{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "70b8da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have TensorFlow version 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "import os, re, string, gzip, itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from keras import utils\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from collections import defaultdict\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, LSTM, Embedding, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4201ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8393509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_score(clf, train_data, train_target, test_data, test_target, gs = True):\n",
    "    clf_ = clf.fit(train_data, train_target)\n",
    "    predicted_test_target = clf_.predict(test_data)\n",
    "    predicted_train_target = clf_.predict(train_data)\n",
    "    if True:\n",
    "        print('For {} classifier accuracy on train data is {}'.format(clf_.steps[-1][-1], accuracy_score(train_target,predicted_train_target)))\n",
    "        print('For {} classifier accuracy on test data is {}'.format(clf_.steps[-1][-1], accuracy_score(test_target, predicted_test_target)))\n",
    "    else:\n",
    "        print('For {} classifier accuracy on train data is {}'.format(clf_, accuracy_score(train_target,predicted_train_target)))\n",
    "        print('For {} classifier accuracy on test data is {}'.format(clf_, accuracy_score(test_target, predicted_test_target)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e87ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words =set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a72ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "398235b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42))])\n",
    "text_clf_stop = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "\n",
    "\n",
    "classfiers = [text_clf, text_clf_svm, text_clf_stop, SVC_pipeline, LogReg_pipeline, NB_pipeline]\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b835ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count =0\n",
    "# for classfier in classfiers:\n",
    "#     count+=1\n",
    "#     print(\"Model : {} \".format(count))\n",
    "#     calc_accuracy_score(classfier, twenty_train.data, twenty_train.target, twenty_test.data, twenty_test.target)\n",
    "#     print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9ecf19d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "gs_clf.best_score_\n",
    "gs_clf.best_params_\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dab184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count =0\n",
    "# for classfier in [gs_clf, gs_clf_svm]:\n",
    "#     count+=1\n",
    "#     print(\"Model : {} \".format(count))\n",
    "#     calc_accuracy_score(classfier, twenty_train.data, twenty_train.target, twenty_test.data, twenty_test.target, gs=False)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('jigsaw-toxic-comment-classification-challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580379c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stop_words]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68615fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f671a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bcf00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfe41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40ff5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f25abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text+Classification+using+python,+scikit+and+nltk.ipynb\r\n",
      "Text+Classification+using+python,+scikit+and+nltk.py\r\n",
      "Text_Classification.ipynb\r\n",
      "classfication.ipynb\r\n",
      "consumer_complaints.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d027bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (5,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Consumer_Complaints.csv', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad243b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccb849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "train_size = int(len(df) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(df) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_narrative = twen\n",
    "train_product = df['Product'][:train_size]\n",
    "\n",
    "test_narrative = df['Consumer complaint narrative'][train_size:]\n",
    "test_product = df['Product'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71bd4734",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_narrative = twenty_train.data\n",
    "train_product = twenty_train.target\n",
    "\n",
    "test_narrative = twenty_test.data\n",
    "test_product = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf3b4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_narrative) # only fit on train\n",
    "x_train = tokenize.texts_to_matrix(train_narrative)\n",
    "x_test = tokenize.texts_to_matrix(test_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80272b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_product)\n",
    "y_train = encoder.transform(train_product)\n",
    "y_test = encoder.transform(test_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8792ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the labels to a one-hot representation\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08539210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (11314, 1000)\n",
      "x_test shape: (7532, 1000)\n",
      "y_train shape: (11314, 20)\n",
      "y_test shape: (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24778874",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "491f8577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 23:34:45.057190: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5483902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 23:34:49.597499: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "319/319 [==============================] - 19s 7ms/step - loss: 2.2704 - accuracy: 0.3393 - val_loss: 0.9573 - val_accuracy: 0.7314\n",
      "Epoch 2/5\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 0.8193 - accuracy: 0.7710 - val_loss: 0.7684 - val_accuracy: 0.7721\n",
      "Epoch 3/5\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 0.5417 - accuracy: 0.8453 - val_loss: 0.7313 - val_accuracy: 0.7836\n",
      "Epoch 4/5\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 0.4035 - accuracy: 0.8877 - val_loss: 0.7382 - val_accuracy: 0.7792\n",
      "Epoch 5/5\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 0.3181 - accuracy: 0.9146 - val_loss: 0.7332 - val_accuracy: 0.7959\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecbb955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 2ms/step - loss: 1.2025 - accuracy: 0.6668\n",
      "Test score: 1.2024712562561035\n",
      "Test accuracy: 0.666755199432373\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0cd7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here's how to generate a prediction on individual examples\n",
    "# text_labels = encoder.classes_ \n",
    "\n",
    "# for i in range(10):\n",
    "#     prediction = model.predict(np.array([x_test[i]]))\n",
    "#     predicted_label = text_labels[np.argmax(prediction)]\n",
    "#     print(test_narrative.iloc[i][:50], \"...\")\n",
    "#     print('Actual label:' + test_product.iloc[i])\n",
    "#     print(\"Predicted label: \" + predicted_label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a1feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19f6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1435e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331916aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a6cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c3f1d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    text = re.sub(pattern=r'<.*?>', repl=' ', string=text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = text.strip()\n",
    "  \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "07dc547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello 123   <br> hghjh</br> completed implementation dèèp lèarning ánd cömputer vísíön satyam@gmail.com , p>>..>>>#9 YUJ hsddu gh ssd [[23/23/344]] https:www.google.com js'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "826fb8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello      hghjh  completed implementation deep learning and computer vision satyamgmailcom  p yuj hsddu gh ssd   js'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801905c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
